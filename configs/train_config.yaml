# DATASET TO USE
data: C://Users/35840/desktop/coding/python/pipeline/datasets/predicting.csv

# RAW DATA PROCESSING
processing:
    resample: true
    time: D
    aggregate: {
        Open: first,
        High: max,
        Low: min,
        Close: last,
        Volume: sum
    }
    label:
        from: Close
        shift: 1

# FEATURE INJECTION
features:
    category: type-1
    window:
        windowtype: default
        timeframe: 14
        windowframes:
            stock: 14
            stocd: 3
            stocsd: 3
            momentum: 10
            roc: 14
            lwr: 14
            disp1: 5
            disp2: 10
            cci: 14
            rsi: 14
            ma: 5
            bias: 6
            psy: 12
            asy5: 5
            asy4: 4
            asy3: 3
            asy2: 2
            asy1: 1

# TRAIN/TEST SPLITTING
splitting:
    train_split: 0.8
    validation_folds: 5

# MODEL ENSEMBLE
regression_ensemble:
    models:

        # LINEAR REGRESSION
        - linreg:

        # LONG SHORT TERM MEMORY
        - lstm:
            morph:
                window: 4
                batch: 30
            layers:
                - lstm:
                    units: 120
                - dropout:
                    rate: 0.15
                - dense:
                    units: 50
                    activation: relu
                - dense:
                    units: 1
            epochs: 7
            loss: mean_squared_error
            optimizer: rmsprop

        # TEMPORAL NEURAL NETWORK
        - tcn:
            morph:
                window: 4 # TCN IS MADE FOR AUDIO, THIS VALUE SHOULD BE EXTREMELY HIGH (~16K)
                batch: 30
            layers:
                - tcn:
                    nb_filters: 100
                    nb_stacks: 1
                    dilations: [1, 2, 4, 8, 16, 32, 64]
                    padding: causal
                    use_skip_connections: False
                    dropout_rate: 0.4
                    return_sequences: False
                - dropout:
                    rate: 0.05
                - dense:
                    units: 30
                    activation: relu
                - dense:
                    units: 1
            epochs: 7
            loss: mean_squared_error
            optimizer: rmsprop

# CLASSIFICATION ENSEMBLE
classification_ensemble:

    # LABEL DECISION
    decision:
        upper: 0.75
        lower: 0.25
    models:

        # RANDOM FOREST
        - randforest:
            static:
                class_weight: balanced
                verbose: 0
            grid_search:
                n_estimators: [10, 50, 100, 300]
                max_features: [log2, sqrt]
                max_depth: [10, 100, none]
                min_samples_split: [3, 10]
                min_samples_leaf: [2, 4]
                bootstrap: [True, False]

        # LOGISTIC REGRESSION
        - logreg:
            static:
                random_state: 0
                class_weight: balanced
                max_iter: 1000
            grid_search:
                C: [0.1, 1, 10, 100]
                penalty: [l2,  none]
                solver: [newton-cg, lbfgs, sag, saga]
                multi_class: [auto, multinomial, ovr]

        # RIDGE CLASSIFIER
        - sgd:
            static:
                epsilon: 0.18
                penalty: l1
                class_weight: balanced
            grid_search:
                eta0: [0.1, 0.5, 1]
                learning_rate: [constant, optimal, invscaling, adaptive]

        # COMPLEMENT NAIVE BAYES
        - naivebays:
            grid_search:
                alpha: [0.5, 1]
                norm: [True, False]