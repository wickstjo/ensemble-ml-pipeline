{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\35840\\Desktop\\coding\\python\\pipeline\\training.ipynb:24: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.training as training\n",
    "import ipynb.fs.full.ensemble as ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.keras as keras\n",
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STORAGE ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_root = 'C://Users/35840/desktop/coding/python/pipeline/storage'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pipeline(params):\n",
    "\n",
    "    # DECONSTRUCT PARAMS\n",
    "    config = params['config']\n",
    "    regression_ensemble = params['regression_ensemble']\n",
    "    classifier_ensemble = params['classifier_ensemble']\n",
    "    predictions = params['predictions']\n",
    "    regression_fitting = params['regression_fitting']\n",
    "    \n",
    "    # USE TIMESTAMP AS NAMING CONVENTION\n",
    "    now = int(time.time())\n",
    "    \n",
    "    # PIPELINE DIR NAME\n",
    "    name = 'PIPELINE-{}'.format(now)\n",
    "    \n",
    "    # DIR PATH\n",
    "    dir_path = '{}/{}'.format(storage_root, name)\n",
    "    \n",
    "    # CREATE NEW DIR\n",
    "    os.mkdir(dir_path)\n",
    "    \n",
    "    # CREATE A PREDICTIONS DIR\n",
    "    os.mkdir('{}/predictions'.format(dir_path))\n",
    "    \n",
    "    # SERIALIZE & SAVE THE YAML CONFIG\n",
    "    save_json(config, '{}/config.json'.format(dir_path))\n",
    "    \n",
    "    # SERIALIZE & SAVE PREDICTIONS & REGERSSION FITTING\n",
    "    save_json(predictions, '{}/predictions.json'.format(dir_path))\n",
    "    save_json(regression_fitting, '{}/regression_fitting.json'.format(dir_path))\n",
    "    \n",
    "    # SAVE REGRESSION ENSEMBLE\n",
    "    save_ensemble(regression_ensemble, 'regression', dir_path)\n",
    "    \n",
    "    # SAVE CLASSIFIER ENSEMBLE\n",
    "    save_ensemble(classifier_ensemble, 'classifier', dir_path)\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE A PIPELINE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(pipeline_name, predictions):\n",
    "    \n",
    "    # DIR PATH\n",
    "    dir_path = '{}/{}'.format(storage_root, pipeline_name)\n",
    "    \n",
    "    # CURRENT TIMESTAMP\n",
    "    now = int(time.time())\n",
    "    \n",
    "    # REFORMAT THE PREDICTIONS\n",
    "    formatted = {\n",
    "        'graph': 'line',\n",
    "        'data': json.loads(predictions.to_json())\n",
    "    }\n",
    "    \n",
    "    # PARSE & SAVE AS JSON\n",
    "    save_json(formatted, '{}/predictions/{}.json'.format(dir_path, now))\n",
    "    \n",
    "    return now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ensemble(ensemble, name, root):\n",
    "    \n",
    "    # SUBPATH\n",
    "    path = '{}/{}_ensemble'.format(root, name)\n",
    "    \n",
    "    # CREATE SUBDIR\n",
    "    os.mkdir(path)\n",
    "    \n",
    "    for model in ensemble.models:\n",
    "        \n",
    "        # MANY MODELS\n",
    "        if type(model) == list:\n",
    "            sub_path = path + '/' + model[0].name + '/'\n",
    "            os.mkdir(sub_path)\n",
    "            \n",
    "            for index, sub in enumerate(model):\n",
    "                final_path = sub_path + str(index)\n",
    "                os.mkdir(final_path)\n",
    "                sub.save(final_path)\n",
    "                \n",
    "        else:\n",
    "            sub_path = path + '/' + model.name + '/'\n",
    "            os.mkdir(sub_path)\n",
    "            model.save(sub_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE/LOAD YAML DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(path):\n",
    "    with open(path, mode='r') as file:\n",
    "        return yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE DATAFRAME AS CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(dataframe, path):\n",
    "    dataframe.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE PICKLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(data, path):\n",
    "    pickle.dump(data, open(path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    return pickle.load(open(path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD & SAVE AS JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, path):\n",
    "    with open(path, 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline(pipeline_name):\n",
    "    \n",
    "    # DIR PATHS\n",
    "    root_path = '{}/{}'.format(storage_root, pipeline_name)\n",
    "    reg_path = '{}/regression_ensemble'.format(root_path)\n",
    "    cls_path = '{}/classifier_ensemble'.format(root_path)\n",
    "    \n",
    "    # SERIALIZE THE CONFIG FILE\n",
    "    config = load_json('{}/config.json'.format(root_path))\n",
    "    \n",
    "    # MODEL NAMES\n",
    "    reg_models = os.listdir(reg_path)\n",
    "    cls_models = os.listdir(cls_path)\n",
    "    \n",
    "    # ENSEMBLES\n",
    "    reg_ensemble = ensemble.create_ensemble()\n",
    "    cls_ensemble = ensemble.create_ensemble()\n",
    "        \n",
    "    # SERIALIZE REGRESSION MODELS\n",
    "    for model_name in reg_models:\n",
    "        temp_path = '{}/{}/'.format(reg_path, model_name)\n",
    "        collection = []\n",
    "        \n",
    "        # PRINT MESSAGE\n",
    "        print('SERIALIZING REGRESSOR {}'.format(model_name.upper()))\n",
    "\n",
    "        # SERIALIZE & APPEND SUBMODELS TO COLLECTION\n",
    "        for sub_model in os.listdir(temp_path):\n",
    "            fin_path  = '{}/{}/'.format(temp_path, sub_model)\n",
    "            model = build_model(fin_path, model_name)\n",
    "            collection.append(model)\n",
    "            \n",
    "            # PRINT MESSAGE\n",
    "            print('\\tFOLD {}'.format(sub_model.upper()))\n",
    "            \n",
    "        # ADD THE COLLECTION TO THE ENSEMBLE\n",
    "        reg_ensemble.add_model(collection)\n",
    "        print()\n",
    "    \n",
    "    # SERIALIZE CLASSIFIER MODELS\n",
    "    for model_name in cls_models:\n",
    "        temp_path = '{}/{}/'.format(cls_path, model_name)\n",
    "        \n",
    "        # BUILD THE MODEL & ADD IT TO THE ENSEMBLE\n",
    "        model = build_model(temp_path, model_name)\n",
    "        cls_ensemble.add_model(model)\n",
    "        \n",
    "        # PRINT MESSAGE\n",
    "        print('SERIALIZED CLASSIFIER {}'.format(model_name.upper()))\n",
    "            \n",
    "    return reg_ensemble, cls_ensemble, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD A MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(path, model_name):\n",
    "    files = set(os.listdir(path))\n",
    "\n",
    "    # LOAD THE SCALER & SETTINGS\n",
    "    scaler = load_pickle(path + 'scaler.pickle')\n",
    "    settings = load_pickle(path + 'settings.pickle')\n",
    "\n",
    "    # LOAD SKLEARN MODEL\n",
    "    if 'model.pickle' in files:\n",
    "        model = load_pickle(path + 'model.pickle')\n",
    "        model = training.basic_model(model, model_name, settings, scaler)\n",
    "\n",
    "    # LOAD KERAS MODEL\n",
    "    else:\n",
    "        model = keras.models.load_model(path + 'model.keras', custom_objects={ 'TCN': TCN })\n",
    "        model = training.generator_model(model_name, settings, scaler, model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD PIPELINE DETAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_details(pipeline_name):\n",
    "    \n",
    "    # PATH\n",
    "    root_path = '{}/{}'.format(storage_root, pipeline_name)\n",
    "    \n",
    "    # CONTAINER\n",
    "    results = {}\n",
    "    \n",
    "    # LOOP THROUGH PREDICTION RESULTS\n",
    "    for file in os.listdir('{}/predictions'.format(root_path)):\n",
    "        \n",
    "        # LOAD CONTENT & EXTRACT NAME\n",
    "        data = load_json('{}/predictions/{}'.format(root_path, file))\n",
    "        file_name = file.split('.')[0]\n",
    "        \n",
    "        # PUSH TO RESULTS\n",
    "        results[file_name] = data\n",
    "    \n",
    "    return {\n",
    "        'predictions': load_json('{}/predictions.json'.format(root_path)),\n",
    "        'regression_fitting': load_json('{}/regression_fitting.json'.format(root_path)),\n",
    "        'config': load_json('{}/config.json'.format(root_path)),\n",
    "        'results': results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
